{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMAOOOOwwwww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Directory to save images\n",
    "image_dir = '/scratch/aabavandpour/LlaVA_img_train'\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Load the original JSON data\n",
    "with open('test_dataset_bert.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare the JSONL output\n",
    "output_lines = []\n",
    "\n",
    "def download_images(img_ids):\n",
    "    local_image_paths = []\n",
    "    for idx, img_id in enumerate(img_ids, start=1):\n",
    "        url = f\"https://xmrec.github.io/mturk_images/all_images/{img_id}\"\n",
    "        local_path = os.path.join(image_dir, img_id)\n",
    "        \n",
    "        # Download and save the image\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with open(local_path, 'wb') as img_file:\n",
    "                img_file.write(response.content)\n",
    "            local_image_paths.append(local_path)\n",
    "            print(f\"Downloaded {idx}/{len(img_ids)}: {img_id}\")\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "    return local_image_paths\n",
    "\n",
    "for entry in data:\n",
    "    conversation_parts = [f\"Topic: {entry['topic']}\"]\n",
    "    all_images = []\n",
    "\n",
    "    for i in range(1, 5):  # Assuming a maximum of 4 turns\n",
    "        question_key = f\"question{i}\"\n",
    "        answer_key = f\"answer{i}\"\n",
    "        img_ids_key = f\"img_ids{i}\"\n",
    "        \n",
    "        if question_key in entry and answer_key in entry:\n",
    "            # Add GPT and USER conversation parts\n",
    "            conversation_parts.append(f\"Gpt: {entry[question_key]}\")\n",
    "            img_ids = entry.get(img_ids_key, [])\n",
    "            conversation_parts.append(' '.join(['<image>' for _ in img_ids]))\n",
    "            all_images.extend(download_images(img_ids))\n",
    "            conversation_parts.append(f\"USER: {entry[answer_key]}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Construct the full conversation\n",
    "    conversation = ' '.join(conversation_parts)\n",
    "\n",
    "    # Prepare the documents\n",
    "    documents = entry.get(\"related_dict\", [])\n",
    "    \n",
    "    # Create the new JSONL entry\n",
    "    new_entry = {\n",
    "        \"images\": all_images,\n",
    "        \"conversation\": conversation,\n",
    "        \"documents\": documents\n",
    "    }\n",
    "    \n",
    "    # Append the JSONL line\n",
    "    output_lines.append(json.dumps(new_entry))\n",
    "\n",
    "# Write to the JSONL file\n",
    "with open('train_dataset_LLaVA.jsonl', 'w') as f:\n",
    "    for line in output_lines:\n",
    "        f.write(line + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alireza",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
